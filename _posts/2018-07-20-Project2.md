---
layout: post
#h2
title: NSF Funding, Soup and Goats <br><br> ## The Secret Formula

---

For my 2nd project, I decided to see if there existed some secret formula that, when configured
just right, could predict the amount of grant funding you could be awarded by the NSF.  I thought
this might be a worthwhile project because: 

* a) Grant proposals are the stuff of nightmares, and are therefore in need of some sort 		     of magical cure, and 
* b) I like the idea of taking a complicated process, throwing it into a
	     regressor, and coming up with a nice linear equation, and transforming the whole	
	     process into a seemingly harmless little collection of coefficients and vectors. 
![alt_text](/pics/Perfect-Grant-Writing-Formula-Cartoon-1024x773.jpg)


To start this project, I had to reach for some soup.  Specifically the beautiful kind, as in BeautifulSoup in python to scrape the NSF website for award summary data: <br><br>
[National Science Foundation Awards Summary](https://dellweb.bfa.nsf.gov/AwdLst2/default.asp) <br><br>
But unfortunately I ran into some issues right away, as the NSF award statistics webpage was configured such that the html code populated their award summary data table only when the user selected a specific State or Institution, which was then sent to their servers as query response data and which generated asp requests to get the corresponding award summary information.  Neither BeautifulSoup nor Selenium (the other web scraping option available in python) were able to automatically generate these requests and responses alone (I would have had to incorporate scrapy as well).

So instead, I used BeautifulSoup to scrape their download data page to retrieve xml files containing award statistics from 2013-2018.  That gave me around 60,000 awards to look at over the 5 year period.

I then started exploring.  Could any of the information contained in these files provide the secret formula for grant funding?  Well, first I needed to see what I had to work with.  I had data on award effective dates and end dates, award amended dates, state, university/institution name, fields (Social sciences, computer science, engineering, etc) and abstracts.  I did some simple pandas calculations to quantify these features: grant lengths, amended lengths, word count of abstracts, and I also pulled in university rankings (by amount of R&D expenditure and total number of graduate students) as well as historical data (number of grant proposals submitted vs number of grants approved).  

### Time to cook!  

I tried a simple linear regression at first.  I split the data (70/30) into a training and testing set and fit the training data to the linear regression model in python.  I then generated predicted award amount values for the features in the testing set.  My model did not predict very well at all, as shown in this plot of predicted vs actual values: ![alt_text](/pics/Modelperf.jpg) 

As my model was fitting so poorly and my RSME values were so high, I decided to do some model diagnostics.  First, I plotted a correlation heatmap and looked at the distribution of the data to get a better sense of how the data varied with abstract funding:
![alt_text](/pics/corrheatmap2.png) 

Then I calculated the variance inflation factor to see if any of the independent features in my dataset should be removed: ![alt_text](/pics/vif.png) While the VIF for Grant Length was not extremely high, it was still over 5, so I re-ran my linear regression model without this feature in the dataset, but it did not make an appreciable difference to the results.

I then looked at the variance of the error in my model by plotting the predicted award amounts against the residuals of the model: ![alt_text](/pics/residuals.png)  In this plot you can clearly see that the distribution of residuals is not random, so the error variance is not constant.

So I decided to try a log transformation of the award amount data to fit into my linear regressor.  In this case, I would be trying to see if any of the features listed above had any significant effect on the log of the award amount data.  Here are the results of my model below:
![alt_text](/pics/exppred.png)
![alt_text](/pics/residuals2.png)
There is now more randomness to the residuals plot, and the regressor did seem to improve, giving RSME scores of 0.92 and R-squared scores of 0.39 for the test sets.  Unfortuntely, there wasn't a clear linear pattern in the relationship between the features and the award amount.  Alas, no secret formula.  

![alt_text](/pics/xkcd_linreg.png)
So instead of a wonderful linear regression model with the magical secret to maximizing your NSF grant funding, I leave you with what's behind door number 3: the Monty Hall Goat!  
![alt_text](/pics/montyhallgoat.png)


 
Thanks!
