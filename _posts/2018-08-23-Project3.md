---
layout: post
#h2
title: OCR for Handwritten Korean Letters 

---

I know that there are tons of OCR (Optical Character Recognition) systems out there, and some of them are fairly decent in terms of accuracy, so I thought I would see if I could build an OCR from scratch for myself, just to see how they're made.  As an extra challenge, I decided to try and build one for the handwritten Korean alphabet instead, to work on something a little different. 

I started off trying to look for a good dataset of images of Korean letters.  While there are tons of images of Korean writing, I needed separate images for each letter, which was harder to find.  On top of that, none of the images that I did find were labelled.  So rather than going down the rabbit hole of multiple hours searching the web, I decided to create one myself.  I took 2210 screenshots of 26 different letters in 85 different fonts, and then used OpenCV to translate the images one pixel in each direction to increase mydatset fivefold to 11,050 images.  
![alt_text](/pics/korean_dataset.png)


### Feature Generation 
Now came the part where I had to figure out what characteristics in these images could be used to differentiate the letters.  I decided to go with basic contour features (area, perimeter, bounding box measurements, aspect ratio, etc) that I obtained through OpenCV.  I had a total of 18 features to start with, but after doing some EDA in SQL, I decided to throw out a few of the features that had less than 1% variance across all images, as I figured they wouldn't help with identifying differences in the letters.  So I ended up with 15 features to put into a classification model.  
![alt_text](/pics/ocr_features.png)

### Classification models
So, even though I tried to narrow my features down to those that would give the most information about how to differentiate the letters, there was still a lot of correlation between features (which makes sense if you think about it--a letter with a large contour is going to have a large area, which also means a large perimeter, etc).  
![alt_text](/pics/tableau_features.jpg)
The image above shows the distribution of features across the dataset.  The colors of the boxes on the top left represent the area of each contour, and the color represents the aspect ratio (ratio of width to height of the bounding box).  The size of the dots represent the radius of the contours, and the colors represent the angle between the major axis of the bounding ellipse and the horizontal line of each image.  The circles at the bottom represent the x/y coordinates of the center of each contour.  The point to take away here is--they all almost look the same!  The sizes of the boxes/dots are not very different, nor are the colors very distinct from each other.  So by looking at this, I knew that logistic regression wasn't going to work well, since the features weren't going to be linearly separable.  Also, since the features were so correlated, Gaussian Naive Bayes wasn't going to work either.  Neither was KNN, as so many of the features were clumped so close together that 'neighborhoods' no longer made sense.  Decision Tree and Random Forest models were giving me huge overfitting issues as the models tried to find splits that made sense for so many correlated features that even when I cut the branches down significantly I still didn't see good classification results.  And with so many classes, SVM wasn't a good choice either.  That left Neural Nets.

 
### Neural Nets to save the day!
![alt_text](/pics/CNN_OCR_results.png)
After wrestling with 6 different classification models (at only 3% accuracy), I spent a few hours setting up a convolutional neural net (with one pooling layer, 1 hidden layer and 1 Softmax layer) to come up with a 91% accuracy rate on my test dataset.  But when you think about it though, a better accuracy rate was expected, because of how much information the model was now receiving.  Initially I was feeding in only a small subset of information about the letters in the images, and you can't get much from contour information about a shape in an image.  It's like covering someone's eyes with a blindfold that has a tiny hole in it and asking them to identify an object in front of them.  With the neural nets, I fed in the entire image, and it was now able to retrieve as much information as it needed to be able to distinguish between each letter.  

### Flask Application
I also wanted to create a nice front end for my project by developing a flask app that allows a user to drag and drop an image of a Korean letter and receive a message that identifies what the letter is.  It uses the neural net that I set up to classify the image based on the weights it learned from my training set.  In the future I'd like to expand this to identifying words, and then perhaps to translate phrases or sentences in an image. 
![alt_text](/pics/flask_ocr.png)

Thanks!
 
Thanks!
